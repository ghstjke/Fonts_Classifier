{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selected font, script will generate each of above character (a-z, A-Z, 0-9) at nine different position to move text by one pixel in right-left and top-bottom direction. \n",
    "\n",
    "#### NOTE: If you want generate some new data use kernel == python2, because ttfquery can cause problems at python3\n",
    "Also you can use generated images from Synthetic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import ttfquery.findsystem \n",
    "import string\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input params:\n",
    "\n",
    "    fontSize\n",
    "    imgSize\n",
    "    position\n",
    "    font_list - list of fonts which will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordbag creating and exporting to .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_parser(inp_str):\n",
    "    lower_case_list = list(string.ascii_lowercase)\n",
    "    words = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    flag = False\n",
    "    while i < len(inp_str):\n",
    "        while inp_str[i] in string.ascii_letters:\n",
    "            i+=1  \n",
    "            flag = True\n",
    "        if flag and i>j+3: # take words with len > 3\n",
    "            words.append(inp_str.lower()[j:i])\n",
    "        i += 1\n",
    "        j = i\n",
    "        flag = False\n",
    "    return words\n",
    "\n",
    "def words_generator(path='words.txt'):\n",
    "    \n",
    "    try:\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        \n",
    "        wordbag = fetch_20newsgroups(subset='all', categories=['sci.space'], shuffle=True, random_state=42)\n",
    "        words = []; i = 0\n",
    "        for text in wordbag.data[:100][:]:\n",
    "            tmp = words_parser(text)\n",
    "            words = np.hstack([words,tmp])\n",
    "        words = words.tolist()\n",
    "        print(len(words), 'words were generated')\n",
    "        f = open('words.txt','w')\n",
    "        for word in words:\n",
    "            f.write(word + '\\n')\n",
    "        f.close()\n",
    "    \n",
    "    except:\n",
    "        print('Install sklearn to generate text!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_bag_reader(filename):\n",
    "    #reading wordbag\n",
    "    words_bag = []\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            words_bag.append(f.read())\n",
    "            words_bag = list(words_bag)\n",
    "        words_bag = words_bag[0].split('\\n')\n",
    "    except:\n",
    "        print(\"File with words doesnt exist!!!\")\n",
    "        \n",
    "    return words_bag\n",
    "\n",
    "def data_generator(fonts, directory='Synthetic_dataset', img_per_class=20, img_size=(128,128), fontSize=20):\n",
    "    \"\"\"\n",
    "    fonst -- list of fonts which will be classaficated\n",
    "    words randomly can be in lower case, upper case and only first char is upper\n",
    "    words are taken from wordbag file\n",
    "    \"\"\"\n",
    "        \n",
    "    words_bag = words_bag_reader('words.txt')\n",
    "    \n",
    "    #creating directory\n",
    "    dataset_path = os.path.join (os.getcwd(), directory)\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path) \n",
    "    \n",
    "    #system fonts\n",
    "    all_fonts = ttfquery.findsystem.findFonts()\n",
    "    #creating paths to fonts\n",
    "    cases = [string.lower, string.upper, string.capitalize]\n",
    "    fonts.sort()\n",
    "    \n",
    "    for cur_font in fonts:\n",
    "        for i in range(img_per_class):\n",
    "            for sys_font in all_fonts:\n",
    "                sys_lower = sys_font.lower()\n",
    "                cur_lower = cur_font.lower()\n",
    "                if cur_lower+'.' in sys_lower:# dot is used to get regular style\n",
    "                    path = sys_font\n",
    "                    font = ImageFont.truetype(path, fontSize)\n",
    "                    text = np.random.choice(words_bag)\n",
    "                    text = np.random.choice(cases)(text)\n",
    "                    text_width, text_height = font.getsize(text)\n",
    "                    flag = (text_width >= img_size[0] or text_height >= img_size[1])\n",
    "                    while flag:\n",
    "                        text = np.random.choice(words_bag)\n",
    "                        text = np.random.choice(cases)(text)\n",
    "                        text_width, text_height = font.getsize(text)\n",
    "                        flag = (text_width >= img_size[0] or text_height >= img_size[1])\n",
    "                    image = Image.new(\"RGB\", (text_width, text_height), (255,255,255))\n",
    "                    draw = ImageDraw.Draw(image)\n",
    "                    draw.text((0,0), text, (0,0,0), font=font)\n",
    "                    file_name = str(cur_lower) + '_' + str(i)+'.jpg'\n",
    "                    file_name = os.path.join(dataset_path,file_name)\n",
    "                    image.save(file_name)\n",
    "                    break   \n",
    "                    \n",
    "def data_generator_folders(fonts, directory='Synthetic_dataset', img_per_class=20, img_size=(128,128), fontSize=20):\n",
    "    \"\"\"\n",
    "    fonst -- list of fonts which will be classaficated\n",
    "    words randomly can be in lower case, upper case and only first char is upper\n",
    "    words are taken from wordbag file\n",
    "    \"\"\"\n",
    "    \n",
    "    words_bag = words_bag_reader('words.txt')\n",
    "        \n",
    "    #creating directory\n",
    "    dataset_path = os.path.join (os.getcwd(), directory)\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path) \n",
    "    \n",
    "    #system fonts\n",
    "    all_fonts = ttfquery.findsystem.findFonts()\n",
    "    #creating paths to fonts\n",
    "    cases = [string.lower, string.upper, string.capitalize]\n",
    "    fonts.sort()\n",
    "    for cur_font in fonts:\n",
    "        for i in range(img_per_class):\n",
    "            for sys_font in all_fonts:\n",
    "                sys_lower = sys_font.lower()\n",
    "                cur_lower = cur_font.lower()\n",
    "                if cur_lower+'.' in sys_lower:# dot is used to get regular style\n",
    "                    path = sys_font\n",
    "                    font = ImageFont.truetype(path, fontSize)\n",
    "                    text = np.random.choice(words_bag)\n",
    "                    text = np.random.choice(cases)(text)\n",
    "                    text_width, text_height = font.getsize(text)\n",
    "                    flag = (text_width >= img_size[0] or text_height >= img_size[1])\n",
    "                    while flag:\n",
    "                        text = np.random.choice(words_bag)\n",
    "                        text = np.random.choice(cases)(text)\n",
    "                        text_width, text_height = font.getsize(text)\n",
    "                        flag = (text_width >= img_size[0] or text_height >= img_size[1])\n",
    "                    image = Image.new(\"RGB\", (text_width, text_height), (255,255,255))\n",
    "                    draw = ImageDraw.Draw(image)\n",
    "                    draw.text((0,0), text, (0,0,0), font=font)\n",
    "                    file_name = str(i)+'.jpg'\n",
    "                    font_dir = os.path.join(dataset_path, cur_lower)\n",
    "                    if not os.path.exists(font_dir):\n",
    "                        os.makedirs(font_dir)\n",
    "                    file_name = os.path.join(font_dir,file_name)\n",
    "                    image.save(file_name)\n",
    "                    break   \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "def data_generator_folders_train_test(fonts, words_bag, directory='Synthetic_dataset', img_per_class=20, \\\n",
    "                                      img_size=(128,128), fontSize=18):\n",
    "    \"\"\"\n",
    "    fonst -- list of fonts which will be classaficated\n",
    "    words randomly can be in lower case, upper case and only first char is upper\n",
    "    words are taken from wordbag file\n",
    "    \"\"\"\n",
    "    #creating directory\n",
    "    dataset_path = os.path.join (os.getcwd(), directory)\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path) \n",
    "    \n",
    "    #system fonts\n",
    "    all_fonts = ttfquery.findsystem.findFonts()\n",
    "    #creating paths to fonts\n",
    "    cases = [string.lower, string.upper, string.capitalize]\n",
    "    fonts.sort()\n",
    "    for cur_font in fonts:\n",
    "        for i in range(img_per_class):\n",
    "            for sys_font in all_fonts:\n",
    "                sys_lower = sys_font.lower()\n",
    "                cur_lower = cur_font.lower()\n",
    "                if cur_lower+'.' in sys_lower:# dot is used to get regular style\n",
    "                    path = sys_font\n",
    "                    font = ImageFont.truetype(path, fontSize)\n",
    "                    text = np.random.choice(words_bag)\n",
    "                    text = np.random.choice(cases)(text)\n",
    "                    text_width, text_height = font.getsize(text)\n",
    "                    flag = (text_width >= img_size[0] or text_height >= img_size[1])\n",
    "                    while flag:\n",
    "                        text = np.random.choice(words_bag)\n",
    "                        text = np.random.choice(cases)(text)\n",
    "                        text_width, text_height = font.getsize(text)\n",
    "                        flag = (text_width >= img_size[0] or text_height >= img_size[1])\n",
    "                    image = Image.new(\"RGB\", (text_width, text_height), (255,255,255))\n",
    "#                     image = Image.new(\"RGB\", img_size, (255,255,255))\n",
    "                    draw = ImageDraw.Draw(image)\n",
    "                    draw.text((0,0), text, (0,0,0), font=font)\n",
    "                    file_name = str(i)+'.jpg'\n",
    "                    font_dir = os.path.join(dataset_path, cur_lower)\n",
    "                    if not os.path.exists(font_dir):\n",
    "                        os.makedirs(font_dir)\n",
    "                    file_name = os.path.join(font_dir,file_name)\n",
    "                    image.save(file_name)\n",
    "                    break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ubuntu\n",
    "fonts_list = ['Arial', 'Verdana', 'Comic_Sans_MS', 'Courier_New', 'Times_New_Roman', 'Impact', 'Georgia', 'Trebuc', \\\n",
    "             'Andalemo', 'Lato-Regular']\n",
    "#for windows\n",
    "fonts_list = ['Arial', 'Verdana', 'Comic', 'Cour', 'Times', 'Impact', 'Georgia', 'Trebuc', \\\n",
    "             'Alfredo', 'Borealis']\n",
    "data_generator_folders(fonts=fonts_list, directory='Synthetic_dataset_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ubuntu\n",
    "fonts_list = ['Arial', 'Verdana', 'Comic_Sans_MS', 'Courier_New', 'Times_New_Roman', 'Impact', 'Georgia', 'Trebuc', \\\n",
    "             'Andalemo', 'Lato-Regular']\n",
    "#for windows\n",
    "fonts_list = ['Arial', 'Verdana', 'Comic', 'Cour', 'Times', 'Impact', 'Georgia', 'Trebuc', \\\n",
    "             'Corbel', 'Alger']\n",
    "img_per_class=100\n",
    "N = len(fonts_list)*img_per_class\n",
    "words_bag = words_bag_reader('words.txt')\n",
    "words_bag_set = np.random.choice(words_bag, size=3*N, replace=False)\n",
    "words_bag_train = words_bag_set[:N]\n",
    "words_bag_val = words_bag_set[N:2*N]\n",
    "words_bag_test = words_bag_set[2*N:]\n",
    "data_generator_folders_train_test(fonts_list, words_bag_train, 'Synthetic_dataset/train/', img_per_class)\n",
    "data_generator_folders_train_test(fonts_list, words_bag_val, 'Synthetic_dataset/val/', img_per_class)\n",
    "data_generator_folders_train_test(fonts_list, words_bag_test, 'Synthetic_dataset/test/', img_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here python3 is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers import Conv2D, ActivityRegularization, MaxPooling2D\n",
    "from keras.backend import resize_images, reshape\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator usage\n",
    "We will use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datagen for train, validation and test\n",
    "\n",
    "For training we use train and validation data.\n",
    "After training, test score is calculated by cross validation at test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(model, directory_train, directory_val, directory_test, batch_size=16, epochs=10, verbose=1,\\\n",
    "            nb_train_samples = 2000, nb_validation_samples = 800, nb_test_samples = 1000, img_size=(128,128)):\n",
    "    \"\"\"\n",
    "    creating train/validaton generators for model fitting\n",
    "    \"\"\"\n",
    "    # this is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255.,)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=directory_train,\n",
    "        batch_size=batch_size,   \n",
    "        color_mode='grayscale',\n",
    "        #save_to_dir='synt_aug/',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        directory=directory_val,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=directory_test,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    " \n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        verbose=verbose,\n",
    "        workers=1,#f you want multiprocessing change it\n",
    "        use_multiprocessing=False,)\n",
    "    \n",
    "    score, acc = model.evaluate_generator(generator=test_generator, steps=nb_test_samples, verbose = 1)\n",
    "    print('Score is', score, acc)\n",
    "    \n",
    "    return history, [score, acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose architecture\n",
    "#### First iteration\n",
    "\n",
    "Architectures:\n",
    "\n",
    "    1) CONV - POOL - DENSE - OUTPUT\n",
    "    2) CONV - CONV - POOL - DENSE - OUTPUT\n",
    "    3) CONV - CONV - CONV - POOL - DENSE - OUTPUT\n",
    "Here we dont use any regularizers, normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (32,32,1)\n",
    "models = []\n",
    "models.append(Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "]))\n",
    "\n",
    "models.append(Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "]))\n",
    "\n",
    "models.append(Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scorelist = []\n",
    "for model in models:\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history, score = datagen(model,'./Synthetic_dataset/train', './Synthetic_dataset/val', './Synthetic_dataset/test',\\\n",
    "                      verbose=0, img_size=(32,32), batch_size=32, epochs=30, nb_train_samples=1000,\\\n",
    "                      nb_validation_samples=200, nb_test_samples = 200)\n",
    "    scorelist.append(score[1])\n",
    "print(scorelist)\n",
    "argmax = np.argmax(scorelist)\n",
    "print('The best model is model ', argmax+1)\n",
    "model = models[argmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use BatchNormalization and Dropout.\n",
    "Parameters will be tuning:\n",
    "\n",
    "    dropout_1 -- uniform in [0, 1]\n",
    "    dropout_2 -- uniform in [0, 1]\n",
    "    second_conv -- adds conv - pool before dense\n",
    "    second_dense -- adds one more dense \n",
    "    dense_units -- choice in [32, 64, 128]\n",
    "    \n",
    "Batch size equals 32, it is good choice for this task.\n",
    "\n",
    "For net tunning we will take model 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hyperas for hyperparameters tunning.\n",
    "There are some preparations for hyperas usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    img_size = (32, 32)\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255.,)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory='./Synthetic_dataset/train',\n",
    "        batch_size=32,   \n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        directory='./Synthetic_dataset/val',\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory='./Synthetic_dataset/test',\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator\n",
    "\n",
    "def model(train_generator, validation_generator, test_generator):\n",
    "    \n",
    "    nb_train_samples = 1000\n",
    "    nb_validation_samples = 200\n",
    "    nb_test_samples = 200\n",
    "    epochs = 30\n",
    "    batch_size = 32\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    conv_second = {{choice(['yes', 'no'])}}\n",
    "    if conv_second == 'yes':\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    dense_second = {{choice(['yes', 'no'])}}\n",
    "    if dense_second == 'yes':\n",
    "        model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        verbose=1,\n",
    "        workers=1,#f you want multiprocessing change it\n",
    "        use_multiprocessing=False,)\n",
    "    \n",
    "    score, acc = model.evaluate_generator(generator=test_generator, steps=nb_test_samples, verbose = 1)\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "train_generator, validation_generator, test_generator = data()\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=30,\n",
    "                                      verbose=1,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='fonts_classifier')\n",
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate_generator(generator=test_generator, steps=200))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_model_top')\n",
    "best_model.save_weights('best_model_top_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_adam = keras.models.load_model('best_model_top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test score of best_model_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255.,)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory='./Synthetic_dataset/test/',\n",
    "    batch_size=32,   \n",
    "    color_mode='grayscale',\n",
    "    target_size=(32,32))\n",
    "acc_adam = []\n",
    "for i in range(5):\n",
    "    acc_adam.append(best_model_adam.evaluate_generator(generator=test_generator, steps=200, verbose = 1)[1])\n",
    "print(np.mean(acc_adam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(best_model.history.history['acc'])\n",
    "plt.plot(best_model.history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(best_model.history.history['loss'])\n",
    "plt.plot(best_model.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is hypothesis that with SGD optimizer we can get better score, but accuracy is so close to 1 and it will be very difficult\n",
    "\n",
    "Lets check it!\n",
    "\n",
    "There is duplicated code, it is necessary for hyperas run correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    img_size = (32, 32)\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255.,)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory='./Synthetic_dataset/train',\n",
    "        batch_size=32,   \n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        directory='./Synthetic_dataset/val',\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory='./Synthetic_dataset/test',\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        target_size=img_size)\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator\n",
    "\n",
    "def model(train_generator, validation_generator, test_generator):\n",
    "    \n",
    "    nb_train_samples = 1000\n",
    "    nb_validation_samples = 200\n",
    "    nb_test_samples = 200\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    conv_second = {{choice(['yes', 'no'])}}\n",
    "    if conv_second == 'yes':\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    dense_second = {{choice(['yes', 'no'])}}\n",
    "    if dense_second == 'yes':\n",
    "        model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "    nesterov = {{choice([True, False])}}\n",
    "    print('nesterov is', nesterov)\n",
    "    sgd = SGD(lr = {{uniform(0, 0.3)}}, momentum = {{uniform(0, 1)}}, nesterov=nesterov)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        verbose=1,\n",
    "        workers=1,#f you want multiprocessing change it\n",
    "        use_multiprocessing=False,)\n",
    "    \n",
    "    score, acc = model.evaluate_generator(generator=test_generator, steps=nb_test_samples, verbose = 1)\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "train_generator, validation_generator, test_generator = data()\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=30,\n",
    "                                      verbose=1,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='fonts_classifier')\n",
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate_generator(generator=test_generator, steps=200))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_weights('best_model_top_sgd_weights')\n",
    "best_model.save('best_model_top_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_sgd = keras.models.load_model('best_model_top_sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test score of best_model_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    acc.append(best_model_sgd.evaluate_generator(generator=test_generator, steps=200, verbose = 1)[1])\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(best_model.history.history['acc'])\n",
    "plt.plot(best_model.history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(best_model.history.history['loss'])\n",
    "plt.plot(best_model.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets try train already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history, score = datagen(best_model_sgd,'./Synthetic_dataset/train', './Synthetic_dataset/val',\\\n",
    "                         './Synthetic_dataset/test', verbose=1, img_size=(32,32), batch_size=32,\\\n",
    "                         epochs=100, nb_train_samples=1000, nb_validation_samples=200, nb_test_samples = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test score of best_model_sgd with one more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sgd = []\n",
    "for i in range(5):\n",
    "    acc_sgd.append(best_model_sgd.evaluate_generator(generator=test_generator, steps=200, verbose = 1)[1])\n",
    "print(np.mean(acc_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_sgd.save_weights('best_model_top_sgd_weights_second')\n",
    "best_model_sgd.save('best_model_top_sgd_second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(best_model_sgd.history.history['acc'])\n",
    "plt.plot(best_model_sgd.history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(best_model_sgd.history.history['loss'])\n",
    "plt.plot(best_model_sgd.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclsuion\n",
    "\n",
    "We trained 2 models with identity architecture:\n",
    "\n",
    "    INPUT - [CONV - CONV - CONV - POOL]*2 - [DENSE]*2 - OUTPUT\n",
    "    \n",
    "One has trained with Adam optimizator, another with SGD.\n",
    "\n",
    "### Results:\n",
    "    \n",
    "With Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy is', np.mean(acc_adam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy is', np.mean(acc_sgd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
