{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selected font, script will generate each of above character (a-z, A-Z, 0-9) at nine different position to move text by one pixel in right-left and top-bottom direction. \n",
    "\n",
    "#### NOTE: If you want generate some new data use kernel == python2, because ttfquery can cause problems at python3\n",
    "Also you can use generated images from Synthetic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import ttfquery.findsystem \n",
    "import string\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input params:\n",
    "\n",
    "    fontSize\n",
    "    imgSize\n",
    "    position\n",
    "    font_list - list of fonts which will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters of font and picture\n",
    "fontSize = 20\n",
    "imgSize = (28,28)\n",
    "position = (0,0)\n",
    "\n",
    "#fonts which will be used\n",
    "fonts_list = ['Arial', 'Verdana', 'Comic_Sans_MS', 'Courier_New', 'Times_New_Roman', 'Impact', 'Georgia', 'Trebuc', \\\n",
    "             'Andalemo', 'Lato-Regular']\n",
    "fonts_list.sort()\n",
    "total_fonts = len(fonts_list)\n",
    "\n",
    "#all images will be stored in 'Synthetic_dataset' directory under current directory\n",
    "dataset_path = os.path.join (os.getcwd(), 'Synthetic_dataset')\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)\n",
    "\n",
    "#creating character list\n",
    "#it can contains lower case chars, upper case chars and digits\n",
    "fhandle = open('Fonts_list.txt', 'r')\n",
    "lower_case_list = list(string.ascii_lowercase)\n",
    "upper_case_list = list(string.ascii_uppercase)\n",
    "digits = range(0,10)\n",
    "digits_list=[str(i) for i in digits]\n",
    "all_char_list = lower_case_list + upper_case_list + digits_list\n",
    "\n",
    "#path to ubuntu fonts\n",
    "#all_fonts = glob.glob(\"/usr/share/fonts/truetype/msttcorefonts/*.ttf\")\n",
    "all_fonts = ttfquery.findsystem.findFonts()\n",
    "f_flag = np.zeros(total_fonts)\n",
    "\n",
    "for sys_font in all_fonts:\n",
    "    font_file = ntpath.basename(sys_font)\n",
    "    font_file = font_file.rsplit('.')\n",
    "    font_file = font_file[0]\n",
    "    f_idx = 0\n",
    "    for font in fonts_list: #use fonts from list\n",
    "        f_lower = font.lower()\n",
    "        s_lower = sys_font.lower()\n",
    "        #check desired font \n",
    "        #use only regular style\n",
    "        if f_lower in s_lower and 'bold' not in s_lower and 'italic' not in s_lower:\n",
    "            #if commented, data will contain all styles of each font\n",
    "            fonts_list.remove(font)\n",
    "            path = sys_font\n",
    "            font = ImageFont.truetype(path, fontSize)\n",
    "            f_flag[f_idx] = 1\n",
    "            for ch in all_char_list:\n",
    "                image = Image.new(\"RGB\", imgSize, (255,255,255))\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                pos_x = 0\n",
    "                pos_y = 0\n",
    "                pos_idx=0\n",
    "                for y in [pos_y-1, pos_y, pos_y+1]:\n",
    "                    for x in [pos_x-1, pos_x, pos_x+1]:\n",
    "                        position = (x,y)\n",
    "                        draw.text(position, ch, (0,0,0), font=font)\n",
    "                        ##without this flag, it creates 'Calibri_a.jpg' even for 'Calibri_A.jpg'\n",
    "                        ##which overwrites lowercase images\n",
    "                        l_u_d_flag = \"u\"\n",
    "                        if ch.islower():\n",
    "                            l_u_d_flag = \"l\"\n",
    "                        elif ch.isdigit():\n",
    "                            l_u_d_flag = \"d\"\n",
    "                        file_name = str(pos_idx) + '_' + font_file + '_' + l_u_d_flag + '_' + ch + '.jpg'\n",
    "                        file_name = os.path.join(dataset_path,file_name)\n",
    "                        image.save(file_name)\n",
    "                        pos_idx = pos_idx + 1\n",
    "        f_idx = f_idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here python3 is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers import Conv2D, MaxPooling1D, ActivityRegularization, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 6} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fonts_list = ['Arial', 'Verdana', 'Comic_Sans_MS', 'Courier_New', 'Times_New_Roman', 'Impact', 'Georgia', 'Trebuc', \\\n",
    "             'Andalemo', 'Lato-Regular']\n",
    "fonts_list.sort()\n",
    "\n",
    "dirName = './Synthetic_dataset/'\n",
    "pics = os.listdir(dirName)\n",
    "imgGen = ImageDataGenerator()\n",
    "fullnames = []\n",
    "xnames = []\n",
    "\n",
    "def data_read(postion='0'):\n",
    "    \"\"\"\n",
    "    position is pixel from char starts\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    for name in pics:\n",
    "        if name[0] == postion: # for only 1 usage of each font with \"0\" position\n",
    "            for i in range(len(fonts_list)):\n",
    "                #creating labels for fonts\n",
    "                font_i_lower = fonts_list[i].lower()\n",
    "                name_lower = name.lower()\n",
    "                if font_i_lower in name_lower:\n",
    "                    y_cur = i\n",
    "                    y.append(y_cur)\n",
    "                    fullname = os.path.join(dirName,name)\n",
    "                    if os.path.isfile(fullname):\n",
    "                        fullnames.append(fullname)\n",
    "                    #loading picture and converting to array\n",
    "                    img = load_img(fullname, color_mode='grayscale')\n",
    "                    #imgGen.standardize(img)\n",
    "                    x = img_to_array(img)\n",
    "                    X.append(x)\n",
    "                    xnames.append(name)\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    X = np.asarray(X)\n",
    "    \n",
    "    return X, y\n",
    "X, y = data_read(postion='0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data to train, validation and test\n",
    "    Train data is 70% of images\n",
    "    Validation data is 15% \n",
    "    Test data is 15%\n",
    "    \n",
    "Also, its not necessary to create validation set, keras can create it from train data, but here it was done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape (434, 28, 28, 1) \n",
      "val_shape (93, 28, 28, 1) \n",
      "test_shape (93, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "y_oh = keras.utils.to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "print('train_shape', X_train.shape, '\\nval_shape', X_val.shape, '\\ntest_shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \"\"\"\n",
    "    params -- dict of hyperparamets which will be tuned\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(params['conv_filt'], (3, 3), activation='sigmoid', input_shape=IMG_SIZE))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(params['dropout_1']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(params['dense_units'], activation='sigmoid', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(params['dropout_2']))\n",
    "    model.add(ActivityRegularization(l2=params['l2_coef']))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datagen for train and valdation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datagen(model, batch_size=16, epochs=10, verbose=1, nb_train_samples = 2000, nb_validation_samples = 800):\n",
    "    \"\"\"\n",
    "    creating train/validaton generators for model fitting\n",
    "    \"\"\"\n",
    "    # this is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.1,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=False)\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255,)\n",
    "\n",
    "    train_generator = train_datagen.flow(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    validation_generator = test_datagen.flow(\n",
    "        x=X_val,\n",
    "        y=y_val,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        verbose=verbose,\n",
    "        workers=1, #if you want multiprocessing change it\n",
    "        use_multiprocessing=False,)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters\n",
    "#### First iteration\n",
    "The aim of this iteration is reduce parameters bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters which will be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sizes = [10,16,32] #grid search\n",
    "epochs = 50\n",
    "# conv filters and dense units will be randomly generated in [filt_lb,filt_rb] and [unit_lb,unit_lb]\n",
    "filt_b = [4, 128]\n",
    "unit_b = [8, 36]\n",
    "IMG_SIZE = X.shape[1:]\n",
    "# dropout random gen in [0,1]\n",
    "drop_1_b = [0,1]\n",
    "drop_2_b = [0,1]\n",
    "#for random l2 ceoff in [1e-2,1e-4]\n",
    "a = np.log10(1e-4)\n",
    "b = np.log10(1e-1)\n",
    "\n",
    "num_experiments = 5 #for each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'conv_filt': 44, 'dropout_1': 0.7222677269241502, 'dense_units': 29, 'dropout_2': 0.63817243223196707, 'l2_coef': 0.0062513735745217472}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) \n",
    "res = []\n",
    "weights = []\n",
    "beta = 0.5\n",
    "best_f1_score = 0.0\n",
    "def fb_score(a,b,beta=2):\n",
    "    tmp = a/b\n",
    "    if tmp>1.0:\n",
    "        tmp=b\n",
    "    return (1.+beta**2)*(a*tmp)/(beta**2 * a+tmp)\n",
    "\n",
    "for batch in batch_sizes:\n",
    "    for num in range(num_experiments):\n",
    "        print(num)    \n",
    "        params = dict(conv_filt=np.random.randint(filt_b[0], filt_b[1]), \\\n",
    "                      dropout_1=(drop_1_b[1]-drop_1_b[0])*np.random.rand()+drop_1_b[0],\\\n",
    "                      dense_units=np.random.randint(unit_b[0], unit_b[1]),\\\n",
    "                      dropout_2=(drop_2_b[1]-drop_2_b[0])*np.random.rand()+drop_2_b[0],\\\n",
    "                      l2_coef=10**((b-a)*np.random.rand()+a))\n",
    "        print(params)\n",
    "        model = create_model(params)\n",
    "        history = datagen(model, batch_size=batch, epochs=epochs, verbose=0, nb_train_samples=1000, nb_validation_samples=250)\n",
    "        score = model.evaluate(X_test/255., y_test, batch_size=batch)\n",
    "        params['batch']=batch\n",
    "        params['score']=score\n",
    "        params['score_train']=[history.history['loss'][-1], history.history['acc'][-1]]\n",
    "        params['score_val']=[history.history['val_loss'][-1], history.history['val_acc'][-1]]\n",
    "#         params['f1_score'] = (1.+beta**2) * (score[1] * history.history['acc'][-1])/(beta**2*score[1] + \\\n",
    "#                                                                                      history.history['acc'][-1])\n",
    "        params['f1_score'] = fb_score(params['score_val'][1], params['score_train'][1], beta=beta)\n",
    "        if params['f1_score'] > best_f1_score:\n",
    "            best_f1_score = params['f1_score']\n",
    "            np.savetxt('acc.txt', history.history['acc'])\n",
    "            np.savetxt('loss.txt', history.history['loss'])\n",
    "            np.savetxt('val_acc.txt', history.history['val_acc'])\n",
    "            np.savetxt('val_loss.txt', history.history['val_loss'])\n",
    "            model.save('model_1_'+str(batch)+'_'+str(num))\n",
    "            print('cur_best_score = ', score, ' cur_best_f1_score = ', best_f1_score)\n",
    "        res.append(params)\n",
    "        print('train score', params['score_train'], '\\n', 'val score', params['score_val'],'\\n', 'test score', score)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select top N models and do new tuning\n",
    "\n",
    "Choose N top models for parameters bounds reducing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How top models will be selected\n",
    "\n",
    "Because all processes is automatic we will check \n",
    "$ F_\\beta = (1+\\beta^2) * \\frac{accuracy_{test} * \\frac{accuracy_{test}}{accuracy_{train}}}{\\beta^2 * accuracy_{test} + \\frac{accuracy_{test}}{accuracy_{train}}}, $\n",
    "where $ \\beta = 0.5, $\n",
    "beacuse we preffer non overfitted model\n",
    "\n",
    "It provides us to select high test accuracy and non overfitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 3 \n",
    "scorelist = [res_i['f1_score'] for res_i in res]\n",
    "scorelist_sorted = sorted(scorelist, reverse=True)\n",
    "toplist_ind = [scorelist.index(scorelist_sorted[i]) for i in range(N)]\n",
    "toplist = [res[i] for i in toplist_ind]\n",
    "params_full = dict(conv_filt=[], dropout_1=[], dense_units=[],\\\n",
    "                    dropout_2=[], l2_coef=[], batch=[], score=[], score_train=[], f1_score=[])\n",
    "for key in params_full.keys():\n",
    "    for i in range(N):\n",
    "        params_full[key].append(toplist[i][key])\n",
    "print(params_full)        \n",
    "#creating new params bounds\n",
    "filt_b = [np.min(params_full['conv_filt']), np.max(params_full['conv_filt'])]\n",
    "unit_b = [np.min(params_full['dense_units']), np.max(params_full['dense_units'])]\n",
    "drop_1_b = [np.min(params_full['dropout_1']), np.max(params_full['dropout_1'])]\n",
    "drop_2_b = [np.min(params_full['dropout_2']), np.max(params_full['dropout_2'])]\n",
    "batch_sizes = np.unique(params_full['batch'])\n",
    "print(batch_sizes)\n",
    "file = open('history.txt', mode='a') #here will be model history\n",
    "#run tunning\n",
    "num_experiments = 10 #for each batch\n",
    "res = []\n",
    "weights = []\n",
    "best_f1_score = 0.0\n",
    "beta = 0.5\n",
    "for batch in batch_sizes:\n",
    "    for num in range(num_experiments):\n",
    "        print(num)    \n",
    "        params_top = dict(conv_filt=np.random.randint(filt_b[0], filt_b[1]), \\\n",
    "                          dropout_1=(drop_1_b[1]-drop_1_b[0])*np.random.rand()+drop_1_b[0],\\\n",
    "                          dense_units=np.random.randint(unit_b[0], unit_b[1]),\\\n",
    "                          dropout_2=(drop_2_b[1]-drop_2_b[0])*np.random.rand()+drop_2_b[0],\\\n",
    "                          l2_coef=10**((b-a)*np.random.rand()+a))\n",
    "        model = create_model(params_top)\n",
    "        history = datagen(model, batch_size=batch, epochs=epochs, verbose=0)\n",
    "        score = model.evaluate(X_test/255., y_test, batch_size=batch)\n",
    "        params_top['batch']=batch\n",
    "        params_top['score']=score\n",
    "        params_top['score_train']=[history.history['loss'][-1], history.history['acc'][-1]]\n",
    "        params_top['score_val']=[history.history['val_loss'][-1], history.history['val_acc'][-1]]\n",
    "#         params_top['f1_score'] = (1.+beta**2) * (history.history['val_acc'][-1] * history.history['acc'][-1])/\\\n",
    "#         (beta**2*history.history['val_acc'][-1] + history.history['acc'][-1])\n",
    "        params_top['f1_score'] = fb_score(params_top['score_val'][1], params_top['score_train'][1], beta=beta)\n",
    "#         params_top['f1_score'] = (1.+beta**2) * (score[1] * history.history['acc'][-1])/(beta**2*score[1] + \\\n",
    "#                                                                                      history.history['acc'][-1])\n",
    "        if params_top['f1_score'] > best_f1_score:\n",
    "            best_f1_score = params_top['f1_score']\n",
    "            np.savetxt('acc_top.txt', history.history['acc'])\n",
    "            np.savetxt('loss_top.txt', history.history['loss'])\n",
    "            np.savetxt('val_acc_top.txt', history.history['val_acc'])\n",
    "            np.savetxt('val_loss_top.txt', history.history['val_loss'])\n",
    "            model.save('model_topN_'+str(batch)+'_'+str(num))\n",
    "            print('cur_best_score = ', score, ' cur_best_f1_score = ', best_f1_score)\n",
    "        res.append(params_top)\n",
    "        print('train score', params['score_train'], '\\n', 'val score', params['score_val'],'\\n', 'test score', score) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load history for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = {'acc': np.loadtxt('acc.txt'), 'loss':np.loadtxt('loss.txt'), 'val_acc': np.loadtxt('val_acc.txt'), \\\n",
    "        'val_loss':np.loadtxt('val_loss.txt')}\n",
    "history = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history['acc'])\n",
    "plt.plot(history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2\n",
    "{'conv_filt': 25, 'dropout_1': 0.7904967222592211, 'dense_units': 32, 'dropout_2': 0.21818281882943158, 'l2_coef': 0.0013172859052098195}\n",
    "93/93 [==============================] - 0s 1ms/step\n",
    "train score [0.36498506812386494, 0.96058091286307057] test score [0.34732017401726017, 0.9462365597806951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2\n",
    "{'conv_filt': 9, 'dropout_1': 0.27587927485933006, 'dense_units': 26, 'dropout_2': 0.44080940296668725, 'l2_coef': 0.00027984322952818714}\n",
    "93/93 [==============================] - 0s 826us/step\n",
    "train score [0.3278759716086368, 0.95435684597838466] test score [0.28196602322721992, 0.92473117702750751]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
